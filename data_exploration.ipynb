{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dff01bb5-cb3c-499b-b454-03f8639a5a6b",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf5d3cd-de45-4790-a9b4-94d6cb5fb3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-29 16:59:09.820444: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image as k_image\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a89033a-a811-48c5-8439-232f78da1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold data\n",
    "data_list = []\n",
    "\n",
    "# Define classes\n",
    "data_path = '/Users/renatoboemer/code/edge/raw_data'\n",
    "for folder in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    \n",
    "    # Check if the item is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        for image in os.listdir(folder_path):\n",
    "            image_path = os.path.join(folder_path, image)\n",
    "            label = folder\n",
    "            data_list.append({'image_path': image_path, 'label': label})\n",
    "\n",
    "# Convert list to DataFrame\n",
    "data = pd.DataFrame(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf994211-1706-4a10-87d0-30e4bf3f2a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/renatoboemer/code/edge/raw_data/cloudy/...</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/renatoboemer/code/edge/raw_data/cloudy/...</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/renatoboemer/code/edge/raw_data/cloudy/...</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/renatoboemer/code/edge/raw_data/cloudy/...</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/renatoboemer/code/edge/raw_data/cloudy/...</td>\n",
       "      <td>cloudy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path   label\n",
       "0  /Users/renatoboemer/code/edge/raw_data/cloudy/...  cloudy\n",
       "1  /Users/renatoboemer/code/edge/raw_data/cloudy/...  cloudy\n",
       "2  /Users/renatoboemer/code/edge/raw_data/cloudy/...  cloudy\n",
       "3  /Users/renatoboemer/code/edge/raw_data/cloudy/...  cloudy\n",
       "4  /Users/renatoboemer/code/edge/raw_data/cloudy/...  cloudy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef547e5-8c9b-49c4-a19e-439838d9490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['cloudy' 'water' 'green_area' 'desert']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "desert        1502\n",
       "cloudy        1500\n",
       "water         1500\n",
       "green_area    1500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Labels: ', data.label.unique())\n",
    "classes = data.label.unique()\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2e008f-cc64-4a86-b1ad-9aee233a5401",
   "metadata": {},
   "source": [
    "## Data Augumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dee4dd9-317b-4fe3-bb04-b9381688b36f",
   "metadata": {},
   "source": [
    "The `desert` class is slightly imbalanced. To balance the dataset by augmenting the `desert` images, we will have to create an additional \n",
    "1500‚àí1131=**369 images**. Let's apply a couple of data augmentation techniques to address this issue:\n",
    "- rotation\n",
    "- translation\n",
    "\n",
    "Given these augmentations, each original `desert` image would generate 7 additional images (3 from rotations and 4 from translations).\n",
    "\n",
    "To achieve 369 new images, we will need to process ‚åà369/7‚åâ=53 original `desert` images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa323ca8-c065-4283-ab05-1f28dae40af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_image(image, angle):\n",
    "    rows, cols, _ = image.shape\n",
    "    M = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1) \n",
    "    return cv2.warpAffine(image, M, (cols, rows))\n",
    "\n",
    "def translate_image(image, x_shift, y_shift):\n",
    "    rows, cols, _ = image.shape\n",
    "    M = np.float32([[1, 0, x_shift], [0, 1, y_shift]])\n",
    "    return cv2.warpAffine(image, M, (cols, rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2b1e25-e927-42eb-9df1-7b4a0345edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to desert images\n",
    "desert_path = '/Users/renatoboemer/code/edge/raw_data/desert'\n",
    "desert_image_paths = []\n",
    "\n",
    "for image in os.listdir(desert_path):\n",
    "    image_path = os.path.join(desert_path, image)\n",
    "    desert_image_paths.append(image_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec7ef68-7b13-4005-83e8-a99c4cbe4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the paths to randomize which images we are augmenting\n",
    "random.shuffle(desert_image_paths)\n",
    "\n",
    "augmented_images = []\n",
    "\n",
    "for i, path in enumerate(desert_image_paths[:53]):\n",
    "    image = cv2.imread(path)\n",
    "\n",
    "    # Extracting the filename without extension\n",
    "    filename = os.path.basename(path)\n",
    "    base_filename, file_extension = os.path.splitext(filename)\n",
    "    \n",
    "    # Rotations\n",
    "    cv2.imwrite(os.path.join(desert_path, f\"{base_filename}_rotated_-15{file_extension}\"), rotate_image(image, -15))\n",
    "    cv2.imwrite(os.path.join(desert_path, f\"{base_filename}_rotated_15{file_extension}\"), rotate_image(image, 15))\n",
    "    cv2.imwrite(os.path.join(desert_path, f\"{base_filename}_rotated_30{file_extension}\"), rotate_image(image, 30))\n",
    "    \n",
    "    # Translations\n",
    "    cv2.imwrite(os.path.join(desert_path, f\"{base_filename}_translated_right{file_extension}\"), translate_image(image, 25, 0))\n",
    "    cv2.imwrite(os.path.join(desert_path, f\"{base_filename}_translated_left{file_extension}\"), translate_image(image, -25, 0))\n",
    "    cv2.imwrite(os.path.join(desert_path, f\"{base_filename}_translated_down{file_extension}\"), translate_image(image, 0, 25))\n",
    "    cv2.imwrite(os.path.join(desert_path, f\"{base_filename}_translated_up{file_extension}\"), translate_image(image, 0, -25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd035a93-2ef7-49c2-8c74-e3a89327b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold data\n",
    "new_data_list = []\n",
    "\n",
    "# Define classes\n",
    "data_path = '/Users/renatoboemer/code/edge/raw_data'\n",
    "for folder in os.listdir(data_path):\n",
    "    folder_path = os.path.join(data_path, folder)\n",
    "    \n",
    "    # Check if the item is a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        for image in os.listdir(folder_path):\n",
    "            if image.endswith(('.jpg', '.png')):\n",
    "                image_path = os.path.join(folder_path, image)\n",
    "                label = folder\n",
    "                new_data_list.append({'image_path': image_path, 'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7146a4f-2b91-4ce5-bff1-84d01b163642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 6002\n",
      "label\n",
      "desert        1502\n",
      "cloudy        1500\n",
      "water         1500\n",
      "green_area    1500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert list to DataFrame\n",
    "df = pd.DataFrame(new_data_list)\n",
    "\n",
    "# Print total count and count per label\n",
    "print(f\"Total images: {len(df)}\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882e67d-5064-427f-88c4-d35c5f2527ff",
   "metadata": {},
   "source": [
    "## Check an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b224b077-8822-4491-b309-792ccff79fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify the path to your image\n",
    "image_path = '/Users/renatoboemer/code/edge/raw_data/desert/desert(42).jpg'\n",
    "\n",
    "# Read the image\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "# Check if the image was loaded successfully\n",
    "if img is None:\n",
    "    print(\"Error: Couldn't load the image.\")\n",
    "else:\n",
    "    # Display the image in the notebook\n",
    "    display(Image(data=cv2.imencode('.jpg', img)[1].tobytes()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b45ade62-d29d-413f-afdc-9b9fba5f16fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image dimensions: (224, 224, 3)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "# Printing the size (height, width, channels)\n",
    "print(\"image dimensions:\", img.shape)\n",
    "\n",
    "# Print image type\n",
    "print(img.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66352538-bc01-4a46-b6fe-bd52a8695442",
   "metadata": {},
   "source": [
    "Images are in 8-bit. Ideally, we would like to train our model using 32-bit to improve accuracy but it will not be possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c45654b-2476-48dc-841e-77eef1319971",
   "metadata": {},
   "source": [
    "## Resize & Normalize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061fd4dc-3d65-48ef-a95b-c2a2e1814c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images_in_directory(main_directory, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Resize all images in the given directory and its sub-directories to the target size.\n",
    "\n",
    "    :param main_directory: Path to the main directory.\n",
    "    :param target_size: Tuple indicating target size (width, height).\n",
    "    \"\"\"\n",
    "    \n",
    "    resized = False  # Flag to indicate if any images were resized\n",
    "    \n",
    "    for root, dirs, files in os.walk(main_directory):\n",
    "        for file in files:\n",
    "            # Check if the file is an image\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read and resize the image\n",
    "                img = cv2.imread(image_path)\n",
    "                if img is not None:  # Check if image was loaded successfully\n",
    "                    resized_img = cv2.resize(img, target_size)\n",
    "                    \n",
    "                    # Save the resized image back to the same path, effectively replacing it\n",
    "                    cv2.imwrite(image_path, resized_img)\n",
    "                    resized = True  # Set the flag to indicate that at least one image was resized\n",
    "\n",
    "    if resized:\n",
    "        print(\"Resizing complete.\")\n",
    "    else:\n",
    "        print(\"No images were resized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf8068-9b0c-4259-948a-5023b74eb273",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = \"/Users/renatoboemer/code/edge/raw_data\"\n",
    "resize_images_in_directory(main_directory, target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e010b4-a467-41b0-8f0f-5b53d3ebbb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(main_directory):\n",
    "    \"\"\"\n",
    "    Normalize pixel values of all images in the given directory and its sub-directories to the [0, 1] range.\n",
    "    \"\"\"\n",
    "    \n",
    "    normalized = False  # Initialize a flag to check if any images were normalized\n",
    "\n",
    "    for root, dirs, files in os.walk(main_directory):\n",
    "        for file in files:\n",
    "            # Check if the file is an image (e.g., has a .jpg or .png extension).\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                image_path = os.path.join(root, file)\n",
    "\n",
    "                # Read the image\n",
    "                img = cv2.imread(image_path)\n",
    "                if img is not None:  # Check if image was loaded successfully\n",
    "                    # Normalize pixel values to the [0, 1] range\n",
    "                    img_normalized = img.astype(np.float32) / 255.0\n",
    "\n",
    "                    # Save the normalized image\n",
    "                    cv2.imwrite(image_path, (img_normalized * 255))\n",
    "\n",
    "                    normalized = True  # Set the flag to indicate that at least one image was normalized\n",
    "\n",
    "    if normalized:\n",
    "        print(\"Normalization complete.\")\n",
    "    else:\n",
    "        print(\"Images not normalized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfc94d-9095-4f93-8db7-d0e36162686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_directory = \"/Users/renatoboemer/code/edge/raw_data\"\n",
    "normalize_images(main_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba064b-2f45-4754-ba60-43394768d7af",
   "metadata": {},
   "source": [
    "## Train, Test, Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6db77088-2867-4c30-a643-b91e493f1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# Split data into train and validation+test sets\n",
    "train_df, val_test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split validation+test set into validation and test sets\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cec5e79-ae78-49f2-9402-0cbb19447634",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'green_area': 0,\n",
    "    'water': 1,\n",
    "    'desert': 2,\n",
    "    'cloudy': 3\n",
    "}\n",
    "\n",
    "train_df['label'] = train_df['label'].map(label_mapping)\n",
    "val_df['label'] = val_df['label'].map(label_mapping)\n",
    "test_df['label'] = test_df['label'].map(label_mapping)\n",
    "\n",
    "train_labels = train_df['label'].values\n",
    "val_labels = val_df['label'].values\n",
    "test_labels = test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6859ae9-74be-4cb5-af59-4dc7cfdbc05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "train_labels_ohe = to_categorical(train_labels, num_classes=4)\n",
    "val_labels_ohe = to_categorical(val_labels, num_classes=4)\n",
    "test_labels_ohe = to_categorical(test_labels, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11419299-994f-4eae-ac31-6d06e9d4fe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  4801\n",
      "Number of validation samples:  600\n",
      "Number of testing samples:  601\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples: \", train_df.shape[0])\n",
    "print(\"Number of validation samples: \", val_df.shape[0])\n",
    "print(\"Number of testing samples: \", test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ff194d2-a4d0-41a3-9ba7-485bea5c5600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image as k_image\n",
    "\n",
    "def load_image(img_path):\n",
    "    \"\"\"Loads the image as a numpy array.\"\"\"\n",
    "    img = k_image.load_img(img_path)\n",
    "    return k_image.img_to_array(img)\n",
    "\n",
    "# Load images for training data\n",
    "train_images = np.array([load_image(img_path) for img_path in train_df['image_path']])\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "# Load images for validation data\n",
    "val_images = np.array([load_image(img_path) for img_path in val_df['image_path']])\n",
    "val_labels = val_df['label'].values\n",
    "\n",
    "# Load images for test data\n",
    "test_images = np.array([load_image(img_path) for img_path in test_df['image_path']])\n",
    "test_labels = test_df['label'].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa6b7d4-432d-4585-9d3c-71262d39d578",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31cfbc3-7dc7-417f-b8d9-d58109af774a",
   "metadata": {},
   "source": [
    "Satellites, especially those with onboard processing for tasks like image classification, have limitations when it comes to power, memory, and computational resources. This contrasts with the extensive resources available on the ground in data centers. Therefore, the convolutional neural networks (CNNs) employed on satellites tend to prioritise efficiency and resource conservation.\n",
    "\n",
    "Given those constraints, I have decided to use [Keras MobileNet](https://keras.io/api/applications/mobilenet/). \n",
    "\n",
    "MobileNet is a class of efficient deep learning neural network architectures designed primarily for mobile and edge devices, rather than server-based applications. The goal of MobileNet is to provide a lightweight model that offers good performance while being computationally efficient, thereby allowing for real-time inference on devices with limited computational resources, such as a satellite üõ∞Ô∏è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ce9870c-d41f-4a60-aa99-f7c877a29c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. Wrap model in a function\n",
    "# def create_model(learning_rate=0.001, dropout_rate=0.2, dense_neurons=128):\n",
    "#     input_shape = (224, 224, 3)\n",
    "#     base_model = MobileNetV3Small(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "\n",
    "#     x = GlobalAveragePooling2D()(base_model.output)\n",
    "#     x = Dense(dense_neurons, activation='relu')(x)\n",
    "#     x = Dropout(dropout_rate)(x)\n",
    "#     output = Dense(4, activation='softmax')(x)\n",
    "\n",
    "#     model = Model(inputs=base_model.input, outputs=output)\n",
    "#     model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "#                   loss=CategoricalCrossentropy(from_logits=False),\n",
    "#                   metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # 2. Create a KerasClassifier\n",
    "# model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=16, verbose=0)\n",
    "\n",
    "# # 3. Define search space for BayesSearchCV\n",
    "# search_space = {\n",
    "#     'learning_rate': Real(0.0001, 0.01, prior='log-uniform'),\n",
    "#     'dropout_rate': Real(0.1, 0.5)\n",
    "#     'dense_neurons': Integer(64, 256)\n",
    "# }\n",
    "\n",
    "# # Initialise BayesSearchCV\n",
    "# bayes_search = BayesSearchCV(estimator=model, \n",
    "#                              search_spaces=search_space,\n",
    "#                              n_iter=10, \n",
    "#                              cv=3, \n",
    "#                              n_jobs=-1, \n",
    "#                              verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9d31bc1-52b3-4a30-9541-d29f8a676a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes_search.fit(train_images, train_labels_ohe,)\n",
    "\n",
    "# best_params = bayes_search.best_params_\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b81eee7d-5006-4077-9f0a-5eb1b39dd89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-29 17:33:32,208] A new study created in memory with name: no-name-394f211d-c442-43f8-92b3-8dd0db597a67\n",
      "/var/folders/0z/4gxpxvr90sg_nm735wh2pcw00000gn/T/ipykernel_42138/879048177.py:33: FutureWarning: KerasPruningCallback has been deprecated in v2.1.0. This feature will be removed in v4.0.0. See https://github.com/optuna/optuna/releases/tag/v2.1.0. Recent Keras release (2.4.0) simply redirects all APIs in the standalone keras package to point to tf.keras. There is now only one Keras: tf.keras. There may be some breaking changes for some workflows by upgrading to keras 2.4.0. Test before upgrading. REF: https://github.com/keras-team/keras/releases/tag/2.4.0. There is an alternative callback function that can be used instead: :class:`~optuna_integration.TFKerasPruningCallback`\n",
      "  callbacks = [KerasPruningCallback(trial, 'val_accuracy')]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "151/151 [==============================] - 53s 323ms/step - loss: 0.1218 - accuracy: 0.9665 - val_loss: 0.0323 - val_accuracy: 0.9933\n",
      "Epoch 2/10\n",
      "151/151 [==============================] - 48s 320ms/step - loss: 0.0363 - accuracy: 0.9875 - val_loss: 0.0106 - val_accuracy: 0.9983\n",
      "Epoch 3/10\n",
      "151/151 [==============================] - 50s 329ms/step - loss: 0.0179 - accuracy: 0.9956 - val_loss: 0.0124 - val_accuracy: 0.9967\n",
      "Epoch 4/10\n",
      "151/151 [==============================] - 50s 332ms/step - loss: 0.0204 - accuracy: 0.9948 - val_loss: 0.0081 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "151/151 [==============================] - 64s 422ms/step - loss: 0.0178 - accuracy: 0.9940 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "151/151 [==============================] - 50s 331ms/step - loss: 0.0137 - accuracy: 0.9958 - val_loss: 0.0085 - val_accuracy: 0.9967\n",
      "Epoch 7/10\n",
      "151/151 [==============================] - 52s 342ms/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 0.0055 - val_accuracy: 0.9983\n",
      "Epoch 8/10\n",
      "151/151 [==============================] - 48s 320ms/step - loss: 0.0113 - accuracy: 0.9967 - val_loss: 0.0064 - val_accuracy: 0.9967\n",
      "Epoch 9/10\n",
      "151/151 [==============================] - 48s 315ms/step - loss: 0.0100 - accuracy: 0.9967 - val_loss: 0.0102 - val_accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "151/151 [==============================] - 49s 325ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0079 - val_accuracy: 0.9967\n",
      "19/19 [==============================] - 9s 460ms/step - loss: 0.0079 - accuracy: 0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-29 17:42:21,261] Trial 0 finished with value: 0.996666669845581 and parameters: {'learning_rate': 0.002687672047407516, 'dropout_rate': 0.14934293517802494, 'dense_neurons': 73}. Best is trial 0 with value: 0.996666669845581.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "151/151 [==============================] - 54s 323ms/step - loss: 0.1358 - accuracy: 0.9517 - val_loss: 0.0299 - val_accuracy: 0.9917\n",
      "Epoch 2/10\n",
      "151/151 [==============================] - 47s 309ms/step - loss: 0.0351 - accuracy: 0.9890 - val_loss: 0.0231 - val_accuracy: 0.9950\n",
      "Epoch 3/10\n",
      "151/151 [==============================] - 56s 372ms/step - loss: 0.0359 - accuracy: 0.9879 - val_loss: 0.0391 - val_accuracy: 0.9883\n",
      "Epoch 4/10\n",
      "151/151 [==============================] - 51s 336ms/step - loss: 0.0268 - accuracy: 0.9917 - val_loss: 0.0205 - val_accuracy: 0.9950\n",
      "Epoch 5/10\n",
      "151/151 [==============================] - 53s 352ms/step - loss: 0.0419 - accuracy: 0.9900 - val_loss: 0.0063 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "151/151 [==============================] - 49s 323ms/step - loss: 0.0381 - accuracy: 0.9915 - val_loss: 0.0235 - val_accuracy: 0.9933\n",
      "Epoch 7/10\n",
      "151/151 [==============================] - 50s 329ms/step - loss: 0.0178 - accuracy: 0.9954 - val_loss: 0.0126 - val_accuracy: 0.9967\n",
      "Epoch 8/10\n",
      "151/151 [==============================] - 50s 333ms/step - loss: 0.0560 - accuracy: 0.9835 - val_loss: 0.0073 - val_accuracy: 0.9967\n",
      "Epoch 9/10\n",
      "151/151 [==============================] - 50s 332ms/step - loss: 0.0155 - accuracy: 0.9956 - val_loss: 0.0226 - val_accuracy: 0.9933\n",
      "Epoch 10/10\n",
      "151/151 [==============================] - 50s 333ms/step - loss: 0.0314 - accuracy: 0.9910 - val_loss: 0.0203 - val_accuracy: 0.9950\n",
      "19/19 [==============================] - 6s 274ms/step - loss: 0.0203 - accuracy: 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-10-29 17:51:11,892] Trial 1 finished with value: 0.9950000047683716 and parameters: {'learning_rate': 0.00924571520427982, 'dropout_rate': 0.17860648246386135, 'dense_neurons': 185}. Best is trial 0 with value: 0.996666669845581.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996666669845581\n",
      "Best hyperparameters: {'learning_rate': 0.002687672047407516, 'dropout_rate': 0.14934293517802494, 'dense_neurons': 73}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.integration import KerasPruningCallback\n",
    "from tensorflow.keras.layers import Dropout, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV3Small\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_model(trial):\n",
    "    # Suggest values for the hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.01, log=True)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.3)\n",
    "    dense_neurons = trial.suggest_int('dense_neurons', 64, 256)\n",
    "    \n",
    "    # Build the model architecture\n",
    "    base_model = MobileNetV3Small(input_shape=(224, 224, 3), include_top=False, weights='imagenet', )\n",
    "    base_model.trainable = False  # Freeze the convolutional base\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(dense_neurons, activation='relu')(x)\n",
    "    predictions = Dense(4, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    model = create_model(trial)\n",
    "\n",
    "    # Add a callback for pruning.\n",
    "    callbacks = [KerasPruningCallback(trial, 'val_accuracy')]\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_images, train_labels_ohe,\n",
    "        validation_data=(val_images, val_labels_ohe),\n",
    "        shuffle=True,\n",
    "        batch_size=32,\n",
    "        epochs=10,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model accuracy on the validation set\n",
    "    val_accuracy = model.evaluate(val_images, val_labels_ohe)[1]\n",
    "    return val_accuracy\n",
    "\n",
    "# Create a study object and specify the direction is 'maximize'.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, timeout=600)\n",
    "\n",
    "# Print the result\n",
    "trial = study.best_trial\n",
    "print(f'Accuracy: {trial.value}')\n",
    "print(f\"Best hyperparameters: {trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50ef71-de67-4f0f-bc91-071beb605ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the optimised model\n",
    "base_model = MobileNetV3Small(input_shape=(224, 224, 3), include_top=False, weights='imagenet', )\n",
    "base_model.trainable = False  # Freeze the convolutional base\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.14934293517802494)(x)\n",
    "x = Dense(73, activation='relu')(x)\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.002687672047407516),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf61bef-0cb7-4615-930f-9b5873c5d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17329541-eac3-45c8-9436-a1bc1e3aa08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels_ohe, \n",
    "                    validation_data=(val_images, val_labels_ohe), \n",
    "                    epochs=10, \n",
    "                    batch_size=160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39c5008-d8e6-46a3-a8f3-e100034596b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867523a-5ba1-4776-a875-712eacb58268",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, test_accuracy = model.evaluate(test_images, test_labels_ohe)\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3748c-acaf-4a1b-9432-4fd30ebbc252",
   "metadata": {},
   "source": [
    "## First version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9846c65-b637-4cb9-82eb-18f91ee718fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define input shape\n",
    "# input_shape = (224, 224, 3)\n",
    "\n",
    "# # Create MobileNetV3-Small model\n",
    "# base_model = MobileNetV3Small(input_shape=input_shape, include_top=False, weights='imagenet')\n",
    "\n",
    "# # Add custom classification layers on top\n",
    "# x = GlobalAveragePooling2D()(base_model.output)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# output = Dense(4, activation='softmax')(x)\n",
    "\n",
    "# # Create the final model\n",
    "# model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "#               loss=CategoricalCrossentropy(from_logits=False),\n",
    "#               metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46193ba-b299-4768-b3f2-e02e9217c5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
